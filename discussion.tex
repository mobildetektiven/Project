\section{Discussion}
    
    The above analysis shows both the possibilities and the challenges of data-driven approaches. The best results shows that it is possible to extract information about the condition of the guide vane system from the data sets. The servo indication dataset yielded the the best results, where all classifiers with a certain accuracy were able to detect change in the system condition. With the commissioning data the classifiers struggled more, and only the one class SVM algorithm showed useful results. It became clear in the analysis of the commissioning data, the amount of information each sample in the training set holds, is vital. In the servo indication case, all data samples have an equal amount of information, since they are all located at the boundary. In the commissioning case, as shown in the Kde plots from Section 3, there is a lot of samples located inside the boundary. Including these samples in the analysis does not only not contribute with any information, they make the fraction of informative samples small, which again makes it hard to create the boundary one wants.   
     
    
    The fact that to find the optimal decision boundary the classifiers output had to be manually checked, indicates what can be an issue with purely data driven models. When you know what you are looking for, this must somehow be incorporated into the learning algorithm, as an heuristic. As could be seen, when normal performance measures such as prediction accuracy and F1-score were used, the resulting classifiers did not yield sufficient decision boundaries. Since manually analyzing the classifiers is very time consuming, only a small subset of the possible  combinations of hyperparameters were analyzed. This means that there might be parameterizations that yields better performance than the once found. 
    
    If the feature dimensionality is higher than two or three dimensions, visual inspection of the decision boundary would become very difficult. Once the visual inspection is no longer an option, the evaluation functions used for the classifiers is the only way to analyze the classifiers performance. As seen in the analysis this could then lead to favouring too complex classifiers. A good example is shown in Figure \ref{fig:nn_servo_overfit}. The performance on the training data is more or less similar to the best classifier, but as can be seen a very worn down turbine will not be classified in the worst class. There is however no easy way to verify this by the numerical evaluations. This could lead to picking models that are too complex, and overfitted on the data. The extreme case is the servo indication, where a random picking of $75 \%$ of the data most likely represents the whole decision boundary, meaning that testing on the test set is literally the same as testing on the training set. Then neither F1-score nor learning curves will be able to tell you that the classifier is overfitting. One possible solution to this could be to introduce artificial data, that checks how the classifier classifies outliers. 
    
    The datasets used in this analysis are not easily obtained at an operational power plant. This analysis shows that when these datasets are available, one can extract information about the condition of the system. However, for a plant under normal operating conditions one can expect that the distribution of the data is even more centered to the middle than the commissioning case. This can lead to problems for the techniques used in this analysis. Such data will need to be analysed before one can say anything about the ability to classify the condition of the guide vanes during operation. Note also that the analysis only shows that it is possible to classify if a turbine is more worn than another. It is not calculating an estimate to the actual friction coefficient of the guide vanes. 
    
    \subsection{Best classifier}
        \subsubsection{Servo indication dataset}
            For the servo indication dataset, the best neural network outperforms multiclass SVM and logistic regression. With an accuracy above $90\%$ and a decision boundary fitting the data very well. This shows what this algorithm can achieve. Note also that it might be possible to create an even better classifier, since only a small subset of the hyperparameters were tested. One class SVM also produced good results for the servo indication set. An important factor to consider is that one class SVM can be used without having samples from turbines with different levels of wear. One can simply create a classifier based on the current data, and as the plant ages one can look at how much of the data is classified as outliers. There is however no way to say anything about the actual condition of the system, only to notify that when its condition is worsening. If data over a longer period is available multiclass classification is an option to consider. An advantage is that a sample is only predicted to one class. In the case of the one class SVM a sample needs to be tested on all classifiers, before one can say anything about which class it belongs to. 
            
            
        \subsubsection{Commissioning dataset}
            For the commissioning data, it is clear that one class SVM is outperforming all the other algorithms. It is the only one which is able  to reproduce similar results to those found for the servo indication data. When comparing the two decision boundaries seen in Figures \ref{fig:all_classes_servo_oneclass} and \ref{fig:all_classes_startup_oneclass} one can see that they have a similar shape. It is also confirmed by looking at how the classifier trained on the commissioning data, predicts the servo indication. From the outlier prediction rate seen in Table \ref{tab:one_svm_outlier} one can see that very similar datasets like A2 and A4 yields an outlier prediction rate of $23\%$, and that the data predictions on A2 and A4 with the A3 classifier, predicts more than half of the samples as outliers. This can indicate that for similar datasets, when the number of outliers increase towards $50\%$, it indicates that the condition of the system has changed quite a bit.
            
            The other algorithms run into problems because of all the samples located in the center of the sample space. The Kde plots yields a good explanation to the difficulties encountered. Most of the samples are located inside the decision boundary for all four turbines, and hence they are overlapping. Firstly this means that they can belong to any of the different classes. Secondly this means that when the classifier is trained it will focus on fitting the most dense parts of the datasets. This is in the middle, not on the outer bounds, which holds the information about the condition of the guide vanes.  
            
            It was also made an attempt to analyze the commissioning data without down-sampling. This lead to a drastic increase in computation time for the different algorithms. Especially for the SVM. This shows that more data is not always the answer, and in this case, as the kde plots shows, it is just more data samples located and distributed the same way. 

        
        % How robust?
        % what can be learned from this?
        
        
        
    
    
    \subsection{Interpertability}
        When working with datadriven models, it is easy to only focus on getting the best possible prediction rate. However, many models are very hard to interpret. Since this analysis was performed with only two different features, visual observation of how the classifications are performed were possible. This is not so easy when the feature set grows above two dimensions. A NN is capable of finding almost any pattern in your data, but it is like a black box. One can observer how accurate it predicts samples, but what actually makes it take the decision it does, is hard to interpret. Here the simpler models such as LR and SVM have an advantage. For the logistic regression case one can simply look at the weights of the different features. Large weight indicates that the variable is important for the prediction, small weight indicates the opposite. For SVM a number of the actual training samples are used as support vectors, and those vectors are used to create the decision boundary. Hence one can look up the samples that are chosen as support vectors and try to identify why they are chosen. These methods gives the programmer a way to interpret what is going in within the classifier, and what features and samples it emphasizes. This is as mentioned much harder for the neural network.
    
    \subsection{Lessons learned}
    
        Data based modelling is not as simple as picking a learning algorithm, feeding your data into it, and it will do a perfect job. Feature engineering is key to a good result. In this analysis the reduction of the feature space into two dimensions, had a huge impact on the end result. This, comes however at the risk of ignoring valuable data. Here is where prior knowledge plays a vital part. Knowing what you are looking for, and where it is most likely to be found can help you a lot. 
        
        Among the chosen algorithms the NN showed a lot of promise, but it was also the algorithm that was hardest to optimize. As mentioned in the analysis, the issue with the seed number used to create random numbers shows one of the challenges. The extreme flexibility a NN has, can make only small changes in the hyperparameters have a big impact. Finding a good network structure is a lot work. The number of sensible possible combinations combined with the fact that the decision boundaries were analyzed manually made it impossible to say if an optimal solution were found. To get the most out of the algorithm an automated rating system for the classifier must be found.  
        