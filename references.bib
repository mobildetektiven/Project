@incollection{Kim2017,
address = {Cham},
author = {Kim, Nam-Ho and An, Dawn and Choi, Joo-Ho},
booktitle = {Prognostics and Health Management of Engineering Systems},
doi = {10.1007/978-3-319-44742-1_1},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim, An, Choi - 2017 - Introduction.pdf:pdf},
pages = {1--24},
publisher = {Springer International Publishing},
title = {{Introduction}},
url = {http://link.springer.com/10.1007/978-3-319-44742-1{\%}7B{\_}{\%}7D1},
year = {2017}
}
@article{Hernandez1998,
author = {Hern{\'{a}}ndez, Mauricio A and Stolfo, Salvatore J},
doi = {10.1023/A:1009761603038},
issn = {13845810},
journal = {Data Mining and Knowledge Discovery},
number = {1},
pages = {9--37},
publisher = {Kluwer Academic Publishers},
title = {{Real-world Data is Dirty: Data Cleansing and The Merge/Purge Problem}},
url = {http://link.springer.com/10.1023/A:1009761603038},
volume = {2},
year = {1998}
}
@incollection{Kim2017,
address = {Cham},
author = {Kim, Nam-Ho and An, Dawn and Choi, Joo-Ho},
booktitle = {Prognostics and Health Management of Engineering Systems},
doi = {10.1007/978-3-319-44742-1_1},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim, An, Choi - 2017 - Introduction.pdf:pdf},
pages = {1--24},
publisher = {Springer International Publishing},
title = {{Introduction}},
url = {http://link.springer.com/10.1007/978-3-319-44742-1{\%}7B{\_}{\%}7D1},
year = {2017}
}
@article{Gola2012,
abstract = {Abstract In oil and gas industries, drilling is a complex and critical operation which require constant and accurate real-time monitoring. To this aim, real-time models are required to provide an overview of the drilling operations when direct and reliable measurements are not available. Given the harsh operating environment, sensor reliability and calibration are critical issues and bad data quality is a typical problem which affects the accuracy of the model. As a result, the driller may be misled about the down-hole situation or receive conflicting claims about operating conditions. This paper presents two approaches based on the use of artificial intelligence to improve monitoring of drilling processes in terms of reduced uncertainty and increased confidence. The first exploits the aggregation of the opinion of different experts within a so-called ensemble approach; the second is based on a so-called grey-box approach which combines a physical model and artificial intelligence. The two approaches are applied to the problem of predicting the bottom-hole pressure during a managed pressure drilling operation to demonstrate the improved accuracy and robustness. 1. Introduction A major critical task when starting the exploitation of a new well is the process of drilling the borehole. Drilling is a close-loop process in which the drilling fluid is pumped into the drill pipe at a pressure enough to cause it to circulate downwardly through the drill pipe, the drill collars, the bit nozzles and upwardly through the annulus between the borehole and the drill pipe back to the surface where it goes through a reconditioning process and is finally re-circulated through the pipes. The purpose of the drilling fluid is to remove rock and sediment fragments produced by the bit during drilling, to transport them to the surface, to cool the bit and to maintain pressure balance against the pressure in the rock formation. For these reasons, it is critical to ensure a constant fluid inlet at the right pressure at all time. A sudden loss of drilling fluid would lead in fact to built-up of rock fragments which, if not detected at an early stage, might lead to much worse situations such as stuck drill pipes, bit failures, drill string twist-off and more, resulting in significant non-productive time due to dangerous fishing trips or, in the worst cases, in the loss of expensive bottom-hole assemblies, potentially in the loss of the entire well and even in blowouts with extreme consequences like large financial losses, severe environmental damages and possible loss of lives. Given the critical role played by the drilling fluid especially in maintaining the correct pressure balance, control systems are adopted to regulate the drilling fluid influx with the purpose of precisely controlling the pressure profiles throughout the well hole. This paper focuses on the so-called Managed Pressure Drilling (MPD) system. The objectives of MPD are to ascertain the well bottom-hole pressure environment limits and to accordingly maintain the annular hydraulic pressure profile within its boundaries, i.e. above the pore pressure of the reservoir or the collapse pressure of the borehole and below the fracturing pressure of the borehole eventually responding rapidly to undesired events. Failure to maintain the correct pressure can for instance result in loss of drilling fluid to the formation or unexpected reservoir influxes (especially gas) which in the worst case scenario can lead to surface blowouts. A simplified model of the MPD control system is sketched in Figure 1. For a detailed description of the MPD system refer to [1].},
author = {Gola, Giulio and Nybo, R and Sui, D and Roverso, D},
doi = {10.2118/150201-MS},
file = {:home/asgeir/Downloads/SPE 150201{\_}nybo{\_}model{\_}enkf{\_}svm{\_}drilling(1).pdf:pdf},
isbn = {9781618399311},
journal = {SPE Intelligent Energy {\ldots}},
number = {March},
pages = {1--7},
title = {{Improving Management and Control of Drilling Operations with Artificial Intelligence}},
url = {https://www.onepetro.org/conference-paper/SPE-150201-MS},
year = {2012}
}
@article{Domingos2012,
author = {Domingos, Pedro and Pedro},
doi = {10.1145/2347736.2347755},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Domingos, Pedro - 2012 - A few useful things to know about machine learning.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
month = {oct},
number = {10},
pages = {78},
publisher = {ACM},
title = {{A few useful things to know about machine learning}},
url = {http://dl.acm.org/citation.cfm?doid=2347736.2347755},
volume = {55},
year = {2012}
}
@article{Hawkins,
abstract = {We consider the problem of finding outliers in large multi-variate databases. Outlier detection can be applied during the data cleans-ing process of data mining to identify problems with the data itself, and to fraud detection where groups of outliers are often of particular inter-est. We use replicator neural networks (RNNs) to provide a measure of the outlyingness of data records. The performance of the RNNs is as-sessed using a ranked score measure. The effectiveness of the RNNs for outlier detection is demonstrated on two publicly available databases.},
author = {Hawkins, Simon and He, Hongxing and Williams, Graham and Baxter, Rohan},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hawkins et al. - Unknown - Outlier Detection Using Replicator Neural Networks.pdf:pdf},
title = {{Outlier Detection Using Replicator Neural Networks}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.12.3366{\&}rep=rep1{\&}type=pdf}
}
@article{Collobert,
abstract = {Convex learning algorithms, such as Sup-port Vector Machines (SVMs), are often seen as highly desirable because they offer strong practical properties and are amenable to the-oretical analysis. However, in this work we show how non-convexity can provide scala-bility advantages over convexity. We show how concave-convex programming can be ap-plied to produce (i) faster SVMs where train-ing errors are no longer support vectors, and (ii) much faster Transductive SVMs.},
author = {Collobert, Ronan and Sinz, Fabian and Weston, Jason and Bottou, L{\'{e}}on},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Collobert et al. - Unknown - Trading Convexity for Scalability.pdf:pdf},
title = {{Trading Convexity for Scalability}},
url = {http://delivery.acm.org/10.1145/1150000/1143870/p201-collobert.pdf?ip=129.241.187.53{\&}id=1143870{\&}acc=ACTIVE SERVICE{\&}key=CDADA77FFDD8BE08.5386D6A7D247483C.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}CFID=832816127{\&}CFTOKEN=92573539{\&}{\_}{\_}acm{\_}{\_}=1511432934{\_}9135dee02b5971768d73a97184b05fca}
}
@article{Baldi1989,
abstract = {-We consider the problem of learning from examples in layered linear feed-forward neural networks using optimization methods, such as back propagation, with respect to the usual quadratic error function E of the connection weights. Our main result is a complete description of the landscape attached to E in terms of principal component analysis. We show that E has a unique minimum corresponding to the projection onto the subspace generated by the first principal vectors of a covariance matrix associated with the training patterns. All the additional critical points of E are saddle points (corresponding to projections onto subspaces generated by higher order vectors). The auto-associative case is examined in detail. Extensions and implications for the learning algorithms are discussed.},
author = {Baldi, Pierre and Hornik, Kurt},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Baldi, Hornik - 1989 - Neural Networks and Principal Component Analysis Learning from Examples Without Local Minima.pdf:pdf},
journal = {Neural Networks},
keywords = {-Neural networks,Back propagation,Learning,Principal component analysis},
pages = {53--58},
title = {{Neural Networks and Principal Component Analysis: Learning from Examples Without Local Minima}},
url = {https://ac.els-cdn.com/0893608089900142/1-s2.0-0893608089900142-main.pdf?{\_}tid=104bc7f4-d038-11e7-88d7-00000aacb35f{\&}acdnat=1511432667{\_}42d620ab61954044a9c22d5eed2dc6f1},
volume = {2},
year = {1989}
}
@article{Scholkopf2001,
abstract = {Suppose you are given some data set drawn from an underlying probability distribution P and you want to estimate a “simple” subset S of input space such that the probability that a test point drawn from P lies outside of S equals some a priori specified value between 0 and 1. We propose a method to approach this problem by trying to estimate a function f that is positive on S and negative on the complement. The functional form of f is given by a kernel expansion in terms of a potentially small subset of the training data; it is regularized by controlling the length of the weight vector in an associated feature space. The expansion coefficients are found by solving a quadratic programming problem, which we do by carrying out sequential optimization over pairs of input patterns. We also provide a theoretical analysis of the statistical performance of our algorithm. The algorithm is a natural extension of the support vector algorithm to the case of unlabeled data.},
author = {Sch{\"{o}}lkopf, Bernhard and Platt, John C. and Shawe-Taylor, John and Smola, Alex J. and Williamson, Robert C.},
doi = {10.1162/089976601750264965},
issn = {0899-7667},
journal = {Neural Computation},
month = {jul},
number = {7},
pages = {1443--1471},
publisher = { MIT Press  238 Main St., Suite 500, Cambridge, MA 02142-1046 USA journals-info@mit.edu  },
title = {{Estimating the Support of a High-Dimensional Distribution}},
url = {http://www.mitpressjournals.org/doi/10.1162/089976601750264965},
volume = {13},
year = {2001}
}
@article{Tax2004a,
abstract = {Data domain description concerns the characterization of a data set. A good description covers all target data but includes no superfluous space. The boundary of a dataset can be used to detect novel data or outliers. We will present the Support Vector Data Description (SVDD) which is inspired by the Support Vector Classifier. It obtains a spherically shaped boundary around a dataset and analogous to the Support Vector Classifier it can be made flexible by using other kernel functions. The method is made robust against outliers in the training set and is capable of tightening the description by using negative examples. We show characteristics of the Support Vector Data Descriptions using artificial and real data.},
author = {Tax, David M J and Fisher, Douglas},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tax, Fisher - 2004 - Support Vector Data Description(2).pdf:pdf},
journal = {Machine Learning},
keywords = {novelty detection,one-class classification,outlier detection,support vector classifier,support vector data description},
pages = {45--66},
title = {{Support Vector Data Description}},
url = {http://mediamatica.ewi.tudelft.nl/sites/default/files/ML{\_}SVDD{\_}04.pdf},
volume = {54},
year = {2004}
}
@article{Ng,
author = {Ng, Andrew},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Unknown - CS229 Additional Notes on Backpropagation.pdf:pdf},
title = {{CS229: Additional Notes on Backpropagation}},
url = {http://cs229.stanford.edu/notes/cs229-notes-backprop.pdf}
}
@article{Nga,
abstract = {We now begin our study of deep learning. In this set of notes, we give an overview of neural networks, discuss vectorization and discuss training neural networks with backpropagation.},
author = {Ng, Andrew},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Ng - Unknown - CS229 Lecture Notes Deep Learning.pdf:pdf},
title = {{CS229 Lecture Notes Deep Learning}},
url = {http://cs229.stanford.edu/notes/cs229-notes-deep{\_}learning.pdf}
}
@book{ChristopherM.Bishop,
author = {{Christopher M. Bishop}},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Christopher M. Bishop - Unknown - Pattern recognition and machine learning.pdf:pdf},
title = {{Pattern recognition and machine learning}},
url = {http://users.isr.ist.utl.pt/{~}wurmd/Livros/school/Bishop - Pattern Recognition And Machine Learning - Springer  2006.pdf}
}
@article{Halbertwhite,
abstract = {-This paper rigorously establishes thut standard rnultiluyer feedforward networks with as f{\&}v us one hidden layer using arbitrary squashing functions ure capable of upproximating uny Bore1 measurable function from one finite dimensional space to another to any desired degree of uccuracy, provided sujficirntly muny hidden units are available. In this sense, multilayer feedforward networks are u class of universul rlpproximators.},
author = {Halbertwhite, Maxwell{\~{}}tinchcombe And},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Halbertwhite - Unknown - Multilayer Feedforward Networks are Universal Approximators.pdf:pdf},
journal = {Neural Networks},
keywords = {-Feedforward networks,Back-propagation networks,Mapping networks,Network representation capability,Sigma-Pi networks,Stone-Weierstrass Theorem Squashing functions,Universal approximation},
pages = {35--366},
title = {{Multilayer Feedforward Networks are Universal Approximators}},
url = {https://ac.els-cdn.com/0893608089900208/1-s2.0-0893608089900208-main.pdf?{\_}tid=2bf292d4-cae4-11e7-ab6e-00000aacb361{\&}acdnat=1510846880{\_}49ac579d7917cbe9b59e14659f1c357e},
volume = {2}
}
@article{Korkov~1992,
abstract = {--Taking advantage of techniques developed by Kolmogorov, we give a direct proof of the universal ap-proximation capabilities of perceptron type networks with two hidden layers. From our proof we derive estimates of numbers of hidden units based on properties of the function being approximated and the accuracy of its approx-imation. Keywords--Feedforward neural networks, Multilayer perceptron type networks, Sigmoidal activation function, Approximations of continuous functions, Uniform approximation, Universal approximation capabilities, Estimates of number of hidden units, Modulus of continuity.},
author = {Korkov{\~{}}, V{\~{}}ra},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Korkov{\~{}} - 1992 - Kolmogorov's Theorem and Multilayer Neural Networks.pdf:pdf},
pages = {501--506},
title = {{Kolmogorov's Theorem and Multilayer Neural Networks}},
url = {https://ac.els-cdn.com/0893608092900128/1-s2.0-0893608092900128-main.pdf?{\_}tid=966cf7d8-cad2-11e7-bbab-00000aab0f6c{\&}acdnat=1510839329{\_}ec0e83cebae14e704b46d386e6522adc},
volume = {5},
year = {1992}
}
@article{Manevitz2007,
abstract = {Automated document retrieval and classification is of central importance in many contexts; our main motivating goal is the efficient classification and retrieval of ''interests'' on the internet when only positive information is available. In this paper, we show how a simple feed-forward neural network can be trained to filter documents under these conditions, and that this method seems to be superior to modified methods (modified to use only positive examples), such as Rocchio, Nearest Neighbor, Naive-Bayes, Distance-based Probability and One-Class SVM algorithms. A novel experimental finding is that retrieval is enhanced substantially in this context by carrying out a certain kind of uniform transformation (''Hadamard'') of the information prior to the training of the network.},
author = {Manevitz, Larry and Yousef, Malik},
doi = {10.1016/j.neucom.2006.05.013},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Manevitz, Yousef - 2007 - One-class document classification via Neural Networks.pdf:pdf},
journal = {Neurocomputing},
pages = {1466--1481},
title = {{One-class document classification via Neural Networks}},
url = {http://cs.haifa.ac.il/{~}manevitz/Publication/One-class document classification via Neural Networks.pdf},
volume = {70},
year = {2007}
}
@article{Hastie,
abstract = {During the past decade there has been an explosion in computation and information tech-nology. With it have come vast amounts of data in a variety of fields such as medicine, biolo-gy, finance, and marketing. The challenge of understanding these data has led to the devel-opment of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It should be a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting—the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression {\&} path algorithms for the lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for " wide " data (p bigger than n), including multiple testing and false discovery rates. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS and invented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.},
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hastie, Tibshirani, Friedman - Unknown - Springer Series in Statistics The Elements of Statistical Learning The Elements of Statistic(2).pdf:pdf},
title = {{Springer Series in Statistics The Elements of Statistical Learning The Elements of Statistical Learning}},
url = {https://web.stanford.edu/{~}hastie/Papers/ESLII.pdf}
}
@techreport{Energi212014,
abstract = {Energi21 er Olje- og energidepartementets strategiorgan for forskning, utvikling og demonstrasjon innen energiomr{\aa}det. Hovedm{\aa}let med Energi21-strategiene er {\aa} gi anbe- falinger til Olje- og energidepartementet om fremtidige prioriteringer for satsingen innen utvikling av nye klima- og milj{\o}vennlige l{\o}sninger for energiomr{\aa}det},
author = {Energi21},
file = {:home/asgeir/Downloads/Energi212014del2web0.pdf:pdf},
title = {{Strategi 2014, del 2}},
url = {https://www.forskningsradet.no/no/Publikasjon/Energi{\_}21{\_}strategi{\_}2014{\_}{\_}Del{\_}2/1253998081130?lang=no},
year = {2014}
}

@article{Choudhury2005,
abstract = {The presence of nonlinearities, e.g., stiction, and deadband in a control valve limits the control loop performance. Stiction is the most commonly found valve problem in the process industry. In spite of many attempts to understand and model the stiction phenomena, there is a lack of a proper model, which can be understood and related directly to the practical situation as observed in real valves in the process industry. This study focuses on the understanding, from real-life data, of the mechanism that causes stiction and proposes a new data-driven model of stiction, which can be directly related to real valves. It also validates the simulation results generated using the proposed model with that from a physical model of the valve. Finally, valuable insights on stiction have been obtained from the describing function analysis of the newly proposed stiction model. r},
annote = {cited 250ish times Page 643 shows a figure of a valve deadbands Deadband: ‘‘In process instrumentation, it is the range through which an input signal may be varied, upon reversal of direction, without initiating an observable change in output signal''},
author = {Choudhury, {\$} M A A Shoukat and Thornhill, N F and Shah, S L},
doi = {10.1016/j.conengprac.2004.05.005},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Choudhury, Thornhill, Shah - 2005 - Modelling valve stiction.pdf:pdf},
journal = {Control Engineering Practice},
keywords = {Backlash,Coulomb friction,Deadband,Deadzone,Hysteresis,Process control,Slip jump,Stickband,Stiction,Viscous friction},
pages = {641--658},
title = {{Modelling valve stiction}},
url = {https://ac.els-cdn.com/S0967066104001145/1-s2.0-S0967066104001145-main.pdf?{\_}tid=c75b9a5c-bfdd-11e7-b665-00000aab0f26{\&}acdnat=1509634671{\_}d40835e745492fd86ead5ddce1a6aed9},
volume = {13},
year = {2005}
}
@article{Kingma,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order mo-ments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpre-tations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical con-vergence properties of the algorithm and provide a regret bound on the conver-gence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
annote = {Adam is a stochastic first order gradient descent based method. It uses advantages from both AdaGrad and RMSprop in its impelentation and is a popular choice for optimization in machine learning. For more in depth information see $\backslash$cite{\{}adam{\}}},
author = {Kingma, Diederik P and Ba, Jimmy Lei},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kingma, Ba - Unknown - ADAM A METHOD FOR STOCHASTIC OPTIMIZATION.pdf:pdf},
title = {{ADAM: A METHOD FOR STOCHASTIC OPTIMIZATION}},
url = {https://arxiv.org/pdf/1412.6980v8.pdf}
}
@article{Duchi2011,
abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic op-timization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal func-tion, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regu-larization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
author = {Duchi, John and Edu, Jduchi@cs Berkeley and Hazan, Elad and Singer, Yoram},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Duchi et al. - 2011 - Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {adaptivity,online learning,stochastic convex optimization,subgradient methods},
pages = {2121--2159},
title = {{Adaptive Subgradient Methods for Online Learning and Stochastic Optimization *}},
url = {http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf},
volume = {12},
year = {2011}
}
@article{Sutskever,
abstract = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful mod-els that were considered to be almost impos-sible to train using stochastic gradient de-scent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum pa-rameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimiza-tion. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks per-form markedly worse when the momentum is absent or poorly tuned. Our success training these models suggests that previous attempts to train deep and re-current neural networks from random initial-izations have likely failed due to poor ini-tialization schemes. Furthermore, carefully tuned momentum methods su for dealing with the curvature issues in deep and recur-rent network training objectives without the need for sophisticated second-order methods.},
author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geo↵rey},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sutskever et al. - Unknown - On the importance of initialization and momentum in deep learning.pdf:pdf},
title = {{On the importance of initialization and momentum in deep learning}},
url = {http://proceedings.mlr.press/v28/sutskever13.pdf}
}
@article{Hernandez1998,
author = {Hern{\'{a}}ndez, Mauricio A. and Stolfo, Salvatore J.},
doi = {10.1023/A:1009761603038},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hern{\'{a}}ndez, Stolfo - 1998 - Real-world Data is Dirty Data Cleansing and The MergePurge Problem.pdf:pdf},
issn = {13845810},
journal = {Data Mining and Knowledge Discovery},
number = {1},
pages = {9--37},
publisher = {Kluwer Academic Publishers},
title = {{Real-world Data is Dirty: Data Cleansing and The Merge/Purge Problem}},
url = {http://link.springer.com/10.1023/A:1009761603038},
volume = {2},
year = {1998}
}
@article{Tax2004,
abstract = {Data domain description concerns the characterization of a data set. A good description covers all target data but includes no superfluous space. The boundary of a dataset can be used to detect novel data or outliers. We will present the Support Vector Data Description (SVDD) which is inspired by the Support Vector Classifier. It obtains a spherically shaped boundary around a dataset and analogous to the Support Vector Classifier it can be made flexible by using other kernel functions. The method is made robust against outliers in the training set and is capable of tightening the description by using negative examples. We show characteristics of the Support Vector Data Descriptions using artificial and real data.},
author = {Tax, David M J and Fisher, Douglas},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Tax, Fisher - 2004 - Support Vector Data Description.pdf:pdf},
journal = {Machine Learning},
keywords = {novelty detection,one-class classification,outlier detection,support vector classifier,support vector data description},
pages = {45--66},
title = {{Support Vector Data Description}},
url = {https://link.springer.com/content/pdf/10.1023{\%}2FB{\%}3AMACH.0000008084.60811.49.pdf},
volume = {54},
year = {2004}
}
@inproceedings{Davis2006,
address = {New York, New York, USA},
annote = {2000+ citations bruke i argumetnasjon for F1 score, og kurver!},
author = {Davis, Jesse and Goadrich, Mark},
booktitle = {Proceedings of the 23rd international conference on Machine learning  - ICML '06},
doi = {10.1145/1143844.1143874},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Davis, Goadrich - 2006 - The relationship between Precision-Recall and ROC curves.pdf:pdf},
isbn = {1595933832},
pages = {233--240},
publisher = {ACM Press},
title = {{The relationship between Precision-Recall and ROC curves}},
url = {http://portal.acm.org/citation.cfm?doid=1143844.1143874},
year = {2006}
}
@article{Davis,
abstract = {Receiver Operator Characteristic (ROC) curves are commonly used to present re-sults for binary decision problems in ma-chine learning. However, when dealing with highly skewed datasets, Precision-Recall (PR) curves give a more informative picture of an algorithm's performance. We show that a deep connection exists between ROC space and PR space, such that a curve dominates in ROC space if and only if it dominates in PR space. A corollary is the notion of an achievable PR curve, which has proper-ties much like the convex hull in ROC space; we show an efficient algorithm for computing this curve. Finally, we also note differences in the two types of curves are significant for algorithm design. For example, in PR space it is incorrect to linearly interpolate between points. Furthermore, algorithms that opti-mize the area under the ROC curve are not guaranteed to optimize the area under the PR curve.},
author = {Davis, Jesse and Goadrich, Mark},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Davis, Goadrich - Unknown - The Relationship Between Precision-Recall and ROC Curves.pdf:pdf},
title = {{The Relationship Between Precision-Recall and ROC Curves}},
url = {http://pages.cs.wisc.edu/{~}jdavis/davisgoadrichcamera2.pdf}
}
@article{Wolfe1959,
author = {Wolfe, Philip and Tucker, A. W.},
doi = {10.2307/1909468},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wolfe, Tucker - 1959 - The Simplex Method for Quadratic Programming(2).pdf:pdf},
issn = {00129682},
journal = {Econometrica},
month = {jul},
number = {3},
pages = {382},
publisher = {The Technology Press of The Massachusetts Institute of Technology, Cambridge, Mass.},
title = {{The Simplex Method for Quadratic Programming}},
url = {http://www.jstor.org/stable/1909468?origin=crossref},
volume = {27},
year = {1959}
}
@article{BergstraJAMESBERGSTRA2012,
abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimiza-tion. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a compar-ison with a large previous study that used grid search and manual search to configure neural net-works and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising con-figuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent " High Throughput " methods achieve surprising success—they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural base-line against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
annote = {Grid search, manual search vs random search. 
How dows grid search work? Grid search tests all possible combinations of your hyperparameters not all hyperparameters are important to tune! Therefore random search is better than grid search.},
author = {{Bergstra JAMESBERGSTRA}, James and {Yoshua Bengio YOSHUABENGIO}, Umontrealca},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bergstra JAMESBERGSTRA, Yoshua Bengio YOSHUABENGIO - 2012 - Random Search for Hyper-Parameter Optimization.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,global optimization,model selection,neural networks,response surface modeling},
pages = {281--305},
title = {{Random Search for Hyper-Parameter Optimization}},
url = {http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf},
volume = {13},
year = {2012}
}
@incollection{Kim2017,
address = {Cham},
author = {Kim, Nam-Ho and An, Dawn and Choi, Joo-Ho},
booktitle = {Prognostics and Health Management of Engineering Systems},
doi = {10.1007/978-3-319-44742-1_1},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kim, An, Choi - 2017 - Introduction.pdf:pdf},
pages = {1--24},
publisher = {Springer International Publishing},
title = {{Introduction}},
url = {http://link.springer.com/10.1007/978-3-319-44742-1{\_}1},
year = {2017}
}
@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}



@misc{chollet2015keras,
author = {François Chollet },
title = {Keras, commit 04e0a10},
year = {2017},
publisher = {GitHub},
journal = {GitHub repository},
howpublished = {\url{https://github.com/fchollet/keras}},
commit = {04e0a10}
}

@article{Abadi,
abstract = {TensorFlow [1] is an interface for expressing machine learn-ing algorithms, and an implementation for executing such al-gorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of hetero-geneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learn-ing systems into production across more than a dozen areas of computer science and other fields, including speech recogni-tion, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the Ten-sorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
author = {Abadi, Mart{\'{i}}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man{\'{e}}, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vi{\'{e}}gas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang and Research, Google},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Abadi et al. - Unknown - TensorFlow Large-Scale Machine Learning on Heterogeneous Distributed Systems.pdf:pdf},
title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
url = {https://arxiv.org/pdf/1603.04467.pdf}
}


@article{Joachims1998,
author = {Joachims, Thorsten},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Joachims - 1998 - Making large-scale SVM learning practical.pdf:pdf},
keywords = {310},
publisher = {Dortmund: SFB 475, Universit{\"{a}}t Dortmund},
title = {{Making large-scale SVM learning practical}},
url = {https://www.econstor.eu/handle/10419/77178},
year = {1998}
}


@article{Kohavi1995,
abstract = {We review accuracy estimation methods and compare the two most common methods: cross-validation and bootstrap. Recent experimen-tal results on artiicial data and theoretical re-sults in restricted settings have shown that for selecting a good classiier from a set of classi-(model selection), ten-fold cross-validation may be better than the more expensive leave-one-out cross-validation. We report on a large-scale experiment|over half a million runs of C4.5 and a Naive-Bayes algorithm|to estimate the eeects of diierent parameters on these al-gorithms on real-world datasets. For cross-validation, we vary the number of folds and whether the folds are stratiied or not; for boot-strap, we vary the number of bootstrap sam-ples. Our results indicate that for real-word datasets similar to ours, the best method to use for model selection is ten-fold stratiied cross validation, even if computation power allows using more folds.},
author = {Kohavi, Ron},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kohavi - 1995 - A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection.pdf:pdf},
title = {{A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection}},
url = {http://robotics.stanford.edu/{~}ronnyk},
year = {1995}
}


@article{machinlearning,
title= {Machine Learning Coursera Video lectures - Andrew Ng},
author= {Andrew Ng},
year= {2017},
url= {https://www.coursera.org/learn/machine-learning/}
}

@article{deepLearning,
title= {Neural Networks and Deep Learning. Coursera Video lectures},
author= {Andrew Ng,Kian Katanforoosh,Younes Bensouda Mourri},
year= {2017},
url= {https://www.coursera.org/learn/neural-networks-deep-learning}
}

@article{Dozat,
abstract = {This work aims to improve upon the recently proposed and rapidly popular-ized optimization algorithm Adam (Kingma {\&} Ba, 2014). Adam has two main components—a momentum component and an adaptive learning rate component. However, regular momentum can be shown conceptually and empirically to be in-ferior to a similar algorithm known as Nesterov's accelerated gradient (NAG). We show how to modify Adam's momentum component to take advantage of insights from NAG, and then we present preliminary evidence suggesting that making this substitution improves the speed of convergence and the quality of the learned mod-els.},
author = {Dozat, Timothy},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dozat - Unknown - Workshop track -ICLR 2016 INCORPORATING NESTEROV MOMENTUM INTO ADAM.pdf:pdf},
title = {{Workshop track -ICLR 2016 INCORPORATING NESTEROV MOMENTUM INTO ADAM}},
url = {https://openreview.net/pdf?id=OM0jvwB8jIp57ZJjtNEZ}
}

@article{Dias2016,
abstract = {a b s t r a c t Since the early 1980s, wind power technology has experienced an immense growth with respect to both the turbine size and market share. As the demand for large-scale wind turbines and lor operation {\&} maintenance cost continues to raise, the interest on condition monitoring system has increased rapidly. The main components of wind turbines are the focus of all CMS since they frequently cause high repair costs and equipment downtime. However, vast quantities of their failures are caused due to a bearing failure. Therefore, bearing condition monitoring becomes crucial. This paper aims at providing a state-of-the-art review on wind turbine bearing condition monitoring techniques such as acoustic measurement, electrical effects monitoring, power quality, temperature monitoring, wear debris analysis and vibration analysis. Furthermore, this paper will present a literature review and discuss several technical, financial and operational challenges from the purchase of the CMS to the wind farm monitoring stage.},
author = {Dias, Henrique and {De Azevedo}, Machado and Ara{\'{u}}jo, Alex Maur{\'{i}}cio and Bouchonneau, Nad{\`{e}}ge},
doi = {10.1016/j.rser.2015.11.032},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dias et al. - 2016 - A review of wind turbine bearing condition monitoring State of the art and challenges.pdf:pdf},
journal = {Renewable and Sustainable Energy Reviews},
keywords = {Condition monitoring,Fault detection,Future challenges,Rolling element bearing,State-of-the-art,Wind turbines},
pages = {368--379},
title = {{A review of wind turbine bearing condition monitoring: State of the art and challenges}},
url = {https://ac.els-cdn.com/S1364032115012976/1-s2.0-S1364032115012976-main.pdf?{\_}tid=1d730dd0-e315-11e7-bb73-00000aab0f6c{\&}acdnat=1513506729{\_}c4b5bddcd08d86d2be8afe542577200e},
volume = {56},
year = {2016}
}

@misc{vannkraft,
  title = {Vannkraftdatabase},
  author = { Caroline Drefvelin },
  howpublished = {\url{https://www.nve.no/energiforsyning-og-konsesjon/vannkraft/vannkraftdatabase/}},
  note = {Accessed: 2017-12-15}
}



@misc{tu,
  title = {Gamle generatorer gir gylne tider},
  howpublished = {\url{https://www.tu.no/artikler/gamle-generatorer-gir-gylne-tider-for-leverandorene/235500}},
  note = {Accessed: 2017-10-12}
}


@article{Dozat,
abstract = {This work aims to improve upon the recently proposed and rapidly popular-ized optimization algorithm Adam (Kingma {\&} Ba, 2014). Adam has two main components—a momentum component and an adaptive learning rate component. However, regular momentum can be shown conceptually and empirically to be in-ferior to a similar algorithm known as Nesterov's accelerated gradient (NAG). We show how to modify Adam's momentum component to take advantage of insights from NAG, and then we present preliminary evidence suggesting that making this substitution improves the speed of convergence and the quality of the learned mod-els.},
author = {Dozat, Timothy},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dozat - Unknown - Workshop track -ICLR 2016 INCORPORATING NESTEROV MOMENTUM INTO ADAM.pdf:pdf},
title = {{Workshop track -ICLR 2016 INCORPORATING NESTEROV MOMENTUM INTO ADAM}},
url = {https://openreview.net/pdf?id=OM0jvwB8jIp57ZJjtNEZ}
}


@article{Stenman2003,
author = {Stenman, A. and Gustafsson, F. and Forsman, K.},
doi = {10.1002/acs.769},
file = {:home/asgeir/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Stenman, Gustafsson, Forsman - 2003 - A segmentation-based method for detection of stiction in control valves(2).pdf:pdf},
issn = {0890-6327},
journal = {International Journal of Adaptive Control and Signal Processing},
keywords = {friction,multi‐model mode estimation,stiction,valve diagnosis},
month = {sep},
number = {7-9},
pages = {625--634},
publisher = {John Wiley {\&} Sons, Ltd.},
title = {{A segmentation-based method for detection of stiction in control valves}},
url = {http://doi.wiley.com/10.1002/acs.769},
volume = {17},
year = {2003}
}


@misc{scikit-web,
  title = {Scikit-learn 0.19.1},
  howpublished = {\url{http://scikit-learn.org/stable/install.html}},
}
